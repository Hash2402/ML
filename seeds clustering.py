# -*- coding: utf-8 -*-
"""3065_3041_ISE2-MicroProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E-57Az9pi5KK2G1d4Rxzqo1IAH3r_CU_

**ISE-2** 

**MICRO PROJECT**

**Name** = 

Harsh Manoj Sharma[3065]

Shubham Datta Gaikwad[3041] 

**Problem Statement** =

Given the dataset of wheat grain seeds 
consisting of three categories. The data is 
unlabelled. Categorize the data in three 
categories.

# AgglomerativeClustering
"""

from google.colab import drive
drive.mount('/content/drive')

#importing required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, normalize
import scipy
from scipy.cluster import hierarchy

import sklearn
from sklearn.cluster import AgglomerativeClustering

import sys
path="/content/sample_data/seeds_dataset.csv"
df=pd.read_csv(path)
#Datset optimization
df.columns=['Area','Perimeter','compactness','length','width','Assymetry coeficient','kernal groove','out']
df['out'] = df['out'].replace(1,0)
df['out'] = df['out'].replace(2,1)
df['out'] = df['out'].replace(3,2)
print(df.head())

#Seperating features
x=df.iloc[:,0:7].values 
y=df.iloc[:,-1]

#Feature Scaling
scaler=StandardScaler()
x=scaler.fit_transform(x)

#Method for Dendrogram based on Datapoints
def plot_dendrogram(clusters):
    plt.figure(figsize=(20,6))
    dendrogram = hierarchy.dendrogram(clusters, orientation="top",leaf_font_size=9, leaf_rotation=360)
    plt.ylabel('Euclidean Distance');
#There are four methods for combining clusters in agglomerative approach. 
#The one we choose to use is called Ward’s Method. Unlike the others. 
#Instead of measuring the distance directly, it analyzes the variance of clusters. 
#Ward’s is said to be the most suitable method for quantitative variables.
#Ward’s method says that the distance between two clusters, A and B, is how much the sum of squares will increase when we merge them:
clusters=hierarchy.linkage(x,method="ward")

#plotting dendrogram
plot_dendrogram(clusters)

#Perform  agglomerative clustering 
clustering =AgglomerativeClustering(n_clusters=3,linkage="ward")
y_pred=clustering.fit_predict(x)


print("\n Predicted output = \n",y_pred)

#Visualization of the clusters
def plot_actual_prediction(x, y, y_pred):
    with plt.style.context(("ggplot", "seaborn")):
        plt.figure(figsize=(17,6))

        plt.subplot(1,2,1)
        plt.scatter(x[y==0,0],x[y==0,1], c = 'red', marker="^", s=50)
        plt.scatter(x[y==1,0],x[y==1,1], c = 'green', marker="^", s=50)
        plt.scatter(x[y==2,0],x[y==2,1], c = 'blue', marker="^", s=50)
        plt.title("Original Cluster")

        plt.subplot(1,2,2)
        plt.scatter(x[y_pred==0,0],x[y_pred==0,1], c = 'red', marker="^", s=50)
        plt.scatter(x[y_pred==1,0],x[y_pred==1,1], c = 'green', marker="^", s=50)
        plt.scatter(x[y_pred==2,0],x[y_pred==2,1], c = 'blue', marker="^", s=50)
        plt.title("Clustering Algorithm Prediction");

plot_actual_prediction(x, y, y_pred)


acc=accuracy_score(y,y_pred)
print("\n Accuracy = ",acc*100)



"""# K means"""

# importing libraries    
import numpy as np   
import matplotlib.pyplot as mtp    
import pandas as pd
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Importing the dataset  
path='/content/sample_data/seeds_dataset.csv'
df = pd.read_csv(path)
#Dataframe optimization
df.columns=['Area','Perimeter','compactness','length','width','Assymetry coeficient','kernal groove','out']  
df['out'] = df['out'].replace(1,0)
df['out'] = df['out'].replace(2,1)
df['out'] = df['out'].replace(3,2)

df.head()

#Seperating Datset values
x = df.iloc[:, 0:7].values
y=df.iloc[:,-1]
#Feature Scaling
scaler=StandardScaler()
x=scaler.fit_transform(x)

#print(x)



#training the K-means model on dataset x  
kmeans = KMeans(n_clusters=3,init='random',random_state= 15)  
y_predict= kmeans.fit_predict(x)

#print(y)
print("\n Predicted output = \n",y_predict)

#visulaization of the clusters
mtp.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s = 100, c = 'blue', label = 'Cluster 1') #for first cluster  
mtp.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s = 100, c = 'green', label = 'Cluster 2') #for second cluster  
mtp.scatter(x[y_predict== 2, 0], x[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3') #for third cluster  
mtp.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroid')   
mtp.title('Clusters of classes')  
mtp.xlabel('Features')  
mtp.ylabel('classes')  
mtp.legend()  
mtp.show()

#Visualization of original clusters
mtp.scatter(x[y == 0, 0], x[y == 0, 1], s = 100, c = 'blue', label = 'Cluster 1')   
mtp.scatter(x[y == 1, 0], x[y == 1, 1], s = 100, c = 'green', label = 'Cluster 2')   
mtp.scatter(x[y== 2, 0], x[y == 2, 1], s = 100, c = 'red', label = 'Cluster 3')  
mtp.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroid')   
mtp.title('Clusters of classes(Original)')  
mtp.xlabel('Features')  
mtp.ylabel('classes')  
mtp.legend()  
mtp.show()

#Accuracy of the model
acc=accuracy_score(y,y_predict)
#print(acc)
print("\n Accuracy= ",acc*100)

li=['knn','ward','single','complete','average']
li1=[91.8,92.8,]